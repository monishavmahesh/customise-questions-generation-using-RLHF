# ğŸš€ RLHF PoC â€” Reinforcement Learning from Human Feedback (Azure OpenAI + Streamlit)

A lightweight proof-of-concept demonstrating the core ideas behind **Reinforcement Learning from Human Feedback (RLHF)** using **Azure OpenAI** for text generation and a **tiny CPU-friendly reward model** to learn human preferences.

Built for **Python 3.11** â€” fully **CPU compatible** (no GPU required).

---

## âœ¨ Features

- Generate multiple AI responses using Azure OpenAI  
-  Collect human preference labels  
-  Train a lightweight reward model on CPU  
-  Rerank AI outputs using predicted human preference  
-  Visualize and download feedback datasets  
-  Secure credential handling via `.env`  
-  Extremely lightweight â€” runs on any laptop  

---

## ğŸ§  Architecture Overview
```
User Prompt â”€â–º Azure GPT (generates responses)
             â”‚
             â–¼
   Human chooses best (Label tab)
             â”‚
             â–¼
 Reward Model learns from feedback
             â”‚
             â–¼
 New outputs reranked by reward score
```

This demonstrates the **core RLHF loop** in a simplified, developer-friendly format.

---

## ğŸ“¦ Project Structure
```
â”œâ”€â”€ ui.py                   # Streamlit app (main UI)
â”œâ”€â”€ reward.py               # Reward model logic
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ preferences.jsonl   # Human feedback data
â”‚   â””â”€â”€ reward_model/       # Trained reward model files
â”œâ”€â”€ .env                    # Azure credentials
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ .gitignore              # Ignore unnecessary files
â””â”€â”€ README.md               # Documentation
```

---

## âš™ï¸ Setup Instructions

### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/<your-username>/rlhf-azure-poc.git
cd rlhf-azure-poc
```
2ï¸âƒ£ Create a Virtual Environment
```
python -m venv .venv
source .venv/bin/activate    # macOS/Linux
.venv\Scripts\activate       # Windows
```

3ï¸âƒ£ Install Dependencies
```
pip install -r requirements.txt
```
4ï¸âƒ£ Configure Azure Environment Variables
Create a .env file in your project root:
```
AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/
AZURE_OPENAI_API_KEY=your-azure-api-key
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o-mini
AZURE_OPENAI_API_VERSION=2024-08-01-preview
```
â–¶ï¸ Run the App
Launch the Streamlit web interface:
```
streamlit run ui.py
```

Then open your browser:
```
http://localhost:8501
```
---

## ğŸ§© Usage Guide
### ğŸ’¬ Generate
Enter any prompt and generate multiple responses.
Example:
â€œExplain Newtonâ€™s First Law to a 10-year-old.â€

### âœ… Label
Select your preferred response and add a reason.
Feedback is stored in:
```
data/preferences.jsonl.
```

### ğŸ“ˆ Train Reward Model

- A minimal RLHF reward model using:
- SentenceTransformers (all-MiniLM-L6-v2) for embeddings
- Logistic Regression for classification
- Runs fully on CPU

### ğŸ… Rerank
Score new AI outputs using the trained reward model and reorder by predicted human preference.

### ğŸ—‚ Dataset
Browse and download all labeled preference data.

### ğŸ§  Reward Model Details
Implemented in reward.py :

- Embeddings: all-MiniLM-L6-v2 (SentenceTransformers)
- Classifier: Logistic Regression (Scikit-learn)
- Device: CPU (safe for low-resource systems)
- Input: (Prompt + Response) text pairs
- Output: Probability of human preference

Model is saved at:
```
data/reward_model/reward_model.joblib
```
Load example:
```
from reward import TinyRewardModel
rm = TinyRewardModel.load("data/reward_model")
```
---

Example Prompts for Testing:

- Category---Example Prompt
- Education---Explain gravity like Iâ€™m 5 years old.
- Professional---Write a thank-you email after a job interview.
- Creative---Describe the ocean as if it could talk.
- Technical---Write a Python function to check if a number is prime.
- Empathy---How would you comfort a friend who failed an exam?

---

ğŸ“œ License
MIT License.
