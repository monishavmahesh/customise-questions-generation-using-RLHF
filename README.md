RLHF PoC â€” Reinforcement Learning from Human Feedback (Azure OpenAI + Streamlit)

A lightweight proof-of-concept demonstrating the principles of Reinforcement Learning from Human Feedback (RLHF) using Azure OpenAI for text generation and a tiny CPU-compatible reward model for learning human preferences.

Built for Python 3.11, works fully on CPU (no GPU required).

ğŸš€ Features

âœ… Generate multiple AI responses for a given prompt using Azure OpenAI
âœ… Label your preferred responses (human feedback collection)
âœ… Train a lightweight reward model (TinyRewardModel) on CPU
âœ… Score and rerank new outputs by predicted human preference
âœ… Visualize and download your preference dataset
âœ… Secure .env-based credential management
âœ… Minimal resource usage â€” ideal for laptops and dev environments

ğŸ§  Architecture Overview

User Prompt â”€â–º Azure GPT (generates responses)
             â”‚
             â–¼
   Human chooses best (Label tab)
             â”‚
             â–¼
 Reward Model learns from feedback
             â”‚
             â–¼
 New outputs reranked by reward score


This project replicates the core RLHF feedback-training loop in a simplified form.

ğŸ“¦ Project Structure
â”œâ”€â”€ ui.py                   # Streamlit app (main UI)
â”œâ”€â”€ reward.py               # Reward model logic
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ preferences.jsonl   # Human feedback data
â”‚   â””â”€â”€ reward_model/       # Trained reward model files
â”œâ”€â”€ .env                    # Azure credentials
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ .gitignore              # Ignore unnecessary files
â””â”€â”€ README.md               # Documentation

âš™ï¸ Setup Instructions

1ï¸âƒ£ Clone the Repository
git clone https://github.com/<your-username>/rlhf-azure-poc.git
cd rlhf-azure-poc

2ï¸âƒ£ Create a Virtual Environment
python -m venv .venv
source .venv/bin/activate    # macOS/Linux
.venv\Scripts\activate       # Windows

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Configure Azure Environment Variables

Create a .env file in your project root:

AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/
AZURE_OPENAI_API_KEY=your-azure-api-key
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o-mini
AZURE_OPENAI_API_VERSION=2024-08-01-preview

â–¶ï¸ Run the App

Launch the Streamlit web interface:
streamlit run ui.py

Then open your browser at:

http://localhost:8501

ğŸ§© Usage Guide

ğŸ’¬ Generate

Enter any prompt and generate multiple responses from your Azure OpenAI model.
Example:

â€œExplain Newtonâ€™s First Law to a 10-year-old.â€

âœ… Label

Select your preferred response and optionally add a reason.
Feedback is stored in data/preferences.jsonl.

ğŸ“ˆ Train Reward Model

Train a lightweight reward model that learns your preferences using:

SentenceTransformers (all-MiniLM-L6-v2) for embeddings

Logistic Regression for classification

ğŸ… Rerank

Use your trained reward model to rank new generations by predicted â€œhuman preference.â€

ğŸ—‚ Dataset

Browse and download all labeled preference data.

ğŸ§  Reward Model Details

Implemented in reward.py :

Embeddings: all-MiniLM-L6-v2 (SentenceTransformers)
Classifier: Logistic Regression (Scikit-learn)
Device: CPU (safe for low-resource systems)

Input: (Prompt + Response) text pairs
Output: Probability of human preference

Model is saved at:
data/reward_model/reward_model.joblib


You can reload it anytime:

from reward import TinyRewardModel
rm = TinyRewardModel.load("data/reward_model")

Example Prompts for Testing:

Category---Example Prompt
Education---Explain gravity like Iâ€™m 5 years old.
Professional---Write a thank-you email after a job interview.
Creative---Describe the ocean as if it could talk.
Technical---Write a Python function to check if a number is prime.
Empathy---How would you comfort a friend who failed an exam?